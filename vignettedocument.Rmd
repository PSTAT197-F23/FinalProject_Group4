---
title: "Clustering Methods"
author: "Manuri Alwis, Kayla Katakis, Kylie Maeda, Jennifer Rink, Jade Thai"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Objectives

-   Explore two different clustering methods: K-means and Hierarchical Clustering

-   Use these clustering methods to analyze California housing data

# K-means clustering

K-means clustering is one of the simplest and most popular unsupervised machine learning algorithms. The objective of k-means is to group similar data points together (into clusters) and find underlying patterns. In order to do so, this algorithm looks for a fixed number of clusters in the data. The "k" in k-means refers to the number of clusters. You will define the target number k, which represents the number of centroids you want in the data. The centroid is the center of the cluster. Every data point is assigned to a cluster through reducing the in-cluster sum of squares. Basically, the k-means algorithm finds k centroids and assigns each data point the nearest cluster, while keeping the centroids as small as possible. The "means" in k-means refers to averaging of the data.

## Set Up

Before diving into clustering, the data must be loaded in along with any necessary packages.

```{r, message=FALSE}
# load packages
library(tidyverse)

# read in data
housing <- read.csv('data/housing.csv', header = TRUE)
head(housing)
```

[short paragraph on normalization here]

```{r}
# normalize data
housing_norm <- scale(housing)
head(housing_norm)
```

## Elbow Method

[paragraph explaining how we use elbow method to pick k]

```{r fig.height= 7, fig.width = 7}
# initialize list of inertias (TSS within)
inertias <- c()

# get inertias for different values of k
set.seed(888)
for (k in 1:15) {
  model <- kmeans(housing_norm, centers = k)
  inertias <- c(inertias, model$tot.withinss)
}

# plot inertia against k
ggplot() +
  geom_line(aes(x = 1:15, y = inertias)) +
  geom_point(aes(x = 1:15, y = inertias)) +
  labs(title = "Elbow Method Plot", x = "Number of Clusters (k)", y = "Inertia")
```

## Modeling

```{r}
# perform k-means clustering with k = 5
kmeans <- kmeans(housing_norm, 5)
```

```{r}
# append cluster assignments to the data (to use for plotting)
housing$cluster <- kmeans$cluster
```

## Interpretation and Visualization

Now, we should take a look at the centroids of each cluster. A centroid is the mean of data points assigned to a cluster. Analyzing the centroids can help us understand the composition of each cluster and how they differ from each other.

```{r}
# display the centroids of each cluster
kmeans$centers
```

Based on the centroids we can loosely infer:

- Cluster 1 seems to group data points with highest `Latitude`, lowest `MedHouseVal`, and relatively low `House Age`. In other words, cluster 1 primarily consists of houses in northern California that tend to be lower in value and age.

- Cluster 2 appears to group points with high `Latitude`, high `HouseAge`, and low `Population` - that is, the least populated, oldest, northern California homes.

- Cluster 3 consists of homes that are the furthest south and relatively low in value. This cluster also has the lowest household median income.

- Cluster 4 contains homes that are typically newest in age and in the most heavily populated areas.

- Cluster 5 groups households with the highest median income, house value, and number of rooms.

Now, let's plot the data using different variables and observe how they are grouped in their clusters. We want to pick variables that show the greatest separation between clusters for visualization purposes.

```{r fig.height= 7, fig.width = 7}
# plot median income against population
ggplot(housing, aes(MedInc, log(Population), color = factor(cluster))) +
  geom_point()
```

The plot above shows the median income plotted against population. We see significant separation of clusters 4 and 5, as cluster 4 has large population values and cluster 5 has large household median income.

Since our data contains latitude and longitude variables, we can actually plot a map of California to visualize where our clusters lie.

```{r fig.height= 7, fig.width = 7}
# clusters on map of california
ggplot(housing, aes(Longitude, Latitude, color = factor(cluster))) +
  geom_point(alpha = 0.5) +
  labs(title = 'Clusters on map of California', color = 'Cluster')
```

Despite some overlap between clusters, this plot shows us that our clusters generally resemble the different geographic and economic regions in California. For example, cluster 5 appears to contain the most wealthy, coastal counties like San Fransisco, Los Angeles, and San Diego, while cluster 2 resembles the greater Bay Area and surrounding areas. Cluster 4 seems to be the more heavily populated areas of Los Angeles and San Diego counties, and clusters 1 and 3 appear to be general separations of northern and southern California.

# Hierarchical Clustering

Hierarchical Clustering organizes data points into a tree-like structure called a dendrogram. The process starts with each data point as an individual cluster and then iteratively merges or splits clusters based on their similarities.
We can use this type of clustering to identify patterns and groupings based not only on geographical and income factors but also on population density for California housing data.

## Set Up
```{r, message=FALSE}
library(MASS)
library(class)
library(factoextra)
library(cluster)
# Set Directory
root_dir <- rprojroot::find_rstudio_root_file()
data_dir <- file.path(root_dir, "data")
setwd(data_dir)
# Read in data
housing <- read_csv('housing.csv')
# Pick just MedInc, Population, Latitude, and Longitude variables
myvars <- c("MedInc", "Population", "Latitude", "Longitude")
subset_housing <- housing[myvars]
# Normalize Data
housing_norm<-as.data.frame(scale(subset_housing))
```

## Distance Method

There are many ways to approach hierarchical clustering, starting with the way we calculate the distance between data points. The method used to calculate the distance metric depends wholly on the type of data being analyzed. For our example, we will be using the `euclidean` distance which works well with continuous data. 

```{r}
d <- dist(housing_norm, method = "euclidean") # distance matrix
```

## Similarity Method

Next, we must decide which similarity method to use to calculate similarities between data points. 

( Insert Table of similarity method here? )

There are general guidelines for each method, but it is a good idea to try a few different types in order to determine which one makes your clusters more interpretable.

We will be using the `ward.D2` method which uses the squared Euclidean distance between cluster centroids to evaluate the increase in variance when merging clusters. In other words, this method making clusters easier to differentiate by minimizing the sum of squared errors.

```{r}
fit <- hclust(d, method="ward.D2")

plot(fit, labels=FALSE) # display dendogram
rect.hclust(fit, k=5)
```
