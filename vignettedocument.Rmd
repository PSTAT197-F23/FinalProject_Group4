---
title: "Clustering Methods"
author: "Manuri Alwis, Kayla Katakis, Kylie Maeda, Jennifer Rink, Jade Thai"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Abstract

## Dataset

The California Housing Dataset used in this analysis project measured by the U.S. Census Bureau in 1990 consists of 20,640 observations, 8 predictives attributes, and a target variable. The predictive attributes include `MedInc` (median income), `HouseAge` (median age of house), `AveRooms` (average number of rooms per household), `AveBedrms` (average number of bedrooms per household), `Population`, `AveOccup` (average number of household members), `Latitude`, and `Longitude` (meant to indicate location of house). The target variable, `MedHouseVal`, measures the median value of houses for California districts, measured in \$100,000s.

Each observation represents one block group, which is the small geographical unit for which the U.S. Census Bureau publishes sample data and usually consists of populations of 600-3,000 people. There are no missing values in this dataset.

## Summary of Published Analysis

# Objectives

-   Explore two different clustering methods: K-means and Hierarchical Clustering

-   Use these clustering methods to analyze California housing data

# K-means clustering

K-means clustering is one of the simplest and most popular unsupervised machine learning algorithms. The objective of k-means is to group similar data points together (into clusters) and find underlying patterns. In order to do so, this algorithm looks for a fixed number of clusters in the data. The "k" in k-means refers to the number of clusters. You will define the target number k, which represents the number of centroids you want in the data. The centroid is the center of the cluster. Every data point is assigned to a cluster through reducing the in-cluster sum of squares. Basically, the k-means algorithm finds k centroids and assigns each data point the nearest cluster, while keeping the centroids as small as possible. The "means" in k-means refers to averaging of the data.

## Set Up

Before diving into clustering, the data must be loaded in along with any necessary packages.

```{r, message=FALSE}
# load packages
library(tidyverse)

# read in data
housing <- read.csv('data/housing.csv', header = TRUE)
head(housing)
```

When clustering, normalizing the data is recommended. Distance-based algorithms, such as k-means and hierarchical, rely on distance metrics. Normalizing the data helps prevent variables with large scales form influencing the distance metrics more than others.

```{r}
# normalize data
housing_norm <- scale(housing)
head(housing_norm)
```

## Elbow Method

The elbow method is a technique used to determine the optimal number of clusters (k) in a dataset for k-means clustering. This method plots the total within-cluster sum of squares against different values of k. It looks for the "elbow" point in the plot. This point is where adding more clusters doesn't significantly reduce the within-cluster sum of squares. The "elbow" point represents a trade-off between better fit to the data and fewer clusters.

```{r fig.height= 7, fig.width = 7}
# initialize list of inertias (TSS within)
inertias <- c()

# get inertias for different values of k
set.seed(888)
for (k in 1:15) {
  model <- kmeans(housing_norm, centers = k)
  inertias <- c(inertias, model$tot.withinss)
}

# plot inertia against k
ggplot() +
  geom_line(aes(x = 1:15, y = inertias)) +
  geom_point(aes(x = 1:15, y = inertias)) +
  labs(title = "Elbow Method Plot", x = "Number of Clusters (k)", y = "Inertia")
```

Look at the plot. Where is the elbow point?

If you said 6, that's correct! This is the point where the rate of decrease sharply changes. We should use this as our value of k.

## Modeling

```{r}
# perform k-means clustering with k = 5
kmeans <- kmeans(housing_norm, 5)
```

```{r}
# append cluster assignments to the data (to use for plotting)
housing$cluster <- kmeans$cluster
```

## Interpretation and Visualization

Now, we should take a look at the centroids of each cluster. A centroid is the mean of data points assigned to a cluster. Analyzing the centroids can help us understand the composition of each cluster and how they differ from each other.

```{r}
# display the centroids of each cluster
kmeans$centers
```

Based on the centroids we can loosely infer:

-   Cluster 1 seems to group data points with highest `Latitude`, lowest `MedHouseVal`, and relatively low `House Age`. In other words, cluster 1 primarily consists of houses in northern California that tend to be lower in value and age.

-   Cluster 2 appears to group points with high `Latitude`, high `HouseAge`, and low `Population` - that is, the least populated, oldest, northern California homes.

-   Cluster 3 consists of homes that are the furthest south and relatively low in value. This cluster also has the lowest household median income.

-   Cluster 4 contains homes that are typically newest in age and in the most heavily populated areas.

-   Cluster 5 groups households with the highest median income, house value, and number of rooms.

Now, let's plot the data using different variables and observe how they are grouped in their clusters. We want to pick variables that show the greatest separation between clusters for visualization purposes.

```{r fig.height= 7, fig.width = 7}
# plot median income against population
ggplot(housing, aes(MedInc, log(Population), color = factor(cluster))) +
  geom_point()
```

The plot above shows the median income plotted against population. We see significant separation of clusters 4 and 5, as cluster 4 has large population values and cluster 5 has large household median income.

Since our data contains latitude and longitude variables, we can actually plot a map of California to visualize where our clusters lie.

```{r fig.height= 7, fig.width = 7}
# clusters on map of california
ggplot(housing, aes(Longitude, Latitude, color = factor(cluster))) +
  geom_point(alpha = 0.5) +
  labs(title = 'Clusters on map of California', color = 'Cluster')
```

Despite some overlap between clusters, this plot shows us that our clusters generally resemble the different geographic and economic regions in California. For example, cluster 5 appears to contain the most wealthy, coastal counties like San Fransisco, Los Angeles, and San Diego, while cluster 2 resembles the greater Bay Area and surrounding areas. Cluster 4 seems to be the more heavily populated areas of Los Angeles and San Diego counties, and clusters 1 and 3 appear to be general separations of northern and southern California.

# Hierarchical Clustering

Hierarchical Clustering organizes data points into a tree-like structure called a dendrogram. The process starts with each data point as an individual cluster and then iteratively merges or splits clusters based on their similarities. We can use this type of clustering to identify patterns and groupings based not only on geographical and income factors but also on population density for California housing data.

## Set Up

```{r, message=FALSE}
library(MASS)
library(class)
library(factoextra)
library(cluster)
# Set Directory
root_dir <- rprojroot::find_rstudio_root_file()
data_dir <- file.path(root_dir, "data")
setwd(data_dir)
# Read in data
housing <- read_csv('housing.csv')
# Pick just MedInc, Population, Latitude, and Longitude variables
myvars <- c("MedInc", "Population", "Latitude", "Longitude")
subset_housing <- housing[myvars]
# Normalize Data
housing_norm<-as.data.frame(scale(subset_housing))
```

## Distance Method

There are many ways to approach hierarchical clustering, starting with the way we calculate the distance between data points. The method used to calculate the distance metric depends wholly on the type of data being analyzed. For our example, we will be using the `euclidean` distance which works well with continuous data.

```{r}
d <- dist(housing_norm, method = "euclidean") # distance matrix
```

## Similarity Method

Next, we must decide which similarity method to use to calculate similarities between data points.

( Insert Table of similarity method here? )

There are general guidelines for each method, but it is a good idea to try a few different types in order to determine which one makes your clusters more interpretable.

We will be using the `ward.D2` method which uses the squared Euclidean distance between cluster centroids to evaluate the increase in variance when merging clusters. In other words, this method making clusters easier to differentiate by minimizing the sum of squared errors.

```{r fig.height= 7, fig.width = 7}
fit <- hclust(d, method="ward.D2")  # fit hierarchical clusters
plot(fit, labels=FALSE) # display dendogram
rect.hclust(fit, k=5)   # display optimal cutoff
```

## CLUSPLOT Visualization

An alternate visualization method for hierarchical clustering is using the `clusplot()` function. This graph allows you to plot the similarities in two-dimensional space, but we can see from the plot below that a three dimensional space would be more suitable for our data.

```{r fig.height= 7, fig.width = 7}
groups <- cutree(fit, k=5)    # cut dendogram 
library(cluster)
# Plot the clusters
clusplot(housing_norm, groups, color=TRUE, shade=TRUE, labels=0, lines=0)
```

## Test Cluster Accuracy with Bootstrapping

In order to determine if the grouping we found is a valid representation of "true" clusters, we randomly resample (with replacement) from our data set and rerun our clustering algorithm. We will be analyzing the bootstrap resamples using the Jaccard Index, or the Similarity Coefficient. It is defined as the size of the intersection divided by the size of the union of the sample sets. We are looking for Average Jaccard Index estimations greater than 0.6 because if the observations cluster the same way most of the time, regardless of the resampling distribution, then the average Jaccard value will be high.

```{r}
library(fpc)
clusters <- 5   # choose optimal number of clusters
#  clus.boot <- clusterboot(housing_norm, 
                        # B=50, # number of bootstrap resamples
                        # clustermethod=hclustCBI, # choose hierarchical clustering method
                        # method="ward.D2", # use the same method as in `hclust()` 
                        # k=clusters, # number of clusters
                        # count=FALSE) # Show progress on screen?

# Restore the object so that knitting the html doesn't take a long time
bootstrapped_cluster<-readRDS(file = "~/FinalProject_Group4/data/HCLUST_bootstrap.rds")

# Rule of thumb: . AvgJaccard <0.6 is unstable . AvgJaccard >0.85 is highly stable
set.seed(12032023)
AvgJaccard <- bootstrapped_cluster$bootmean
Instability <- bootstrapped_cluster$bootbrd/1000
Clusters <- c(1:clusters)
Eval <- cbind(Clusters, AvgJaccard, Instability)
Eval
```

## 
